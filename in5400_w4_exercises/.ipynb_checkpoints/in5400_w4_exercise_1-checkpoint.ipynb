{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Coding exercise of the week\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will walk you through the basics of PyTorch. The goal is for you to become familiar with:\n",
    "- What is a computational graph\n",
    "- How to define a pytorch data loader\n",
    "- How to build a neural network in pytorch\n",
    "- How to define a pytorch optimizer\n",
    "- How to train a neural network in pytorch\n",
    "\n",
    "\n",
    "We will assume the following software versions, but other versions may also work:\n",
    "- Python 3.8 (at least 3.6 should also work)\n",
    "- PyTorch 1.7.1\n",
    "\n",
    "\n",
    "In addition to this exercise, we recommend you to have a look at the official tutorials on pytorch.org\n",
    "\n",
    "https://pytorch.org/tutorials/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Play with MNIST Fashion and PyTorch\n",
    "\n",
    "---\n",
    "Your task is to use PyTorch to build a model and train a neural network on the MNIST Fashion dataset. Before you can start, you need to have access to the MNIST Fashion dataset. If you use an IFI computer, the default path given in this Jupyter Notebook file will root you to the data. If you work on any other computer, you will need to download the MNIST Fashion dataset. You can download the files from: https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion\n",
    "\n",
    "MINST Fashion files:\n",
    "- t10k-images-idx3-ubyte.gz\n",
    "- t10k-labels-idx1-ubyte.gz\n",
    "- train-images-idx3-ubyte.gz\n",
    "- train-labels-idx1-ubyte.gz\n",
    "\n",
    "Do not download the files with rightclick-save as in GitHub, but e.g. with left clicking and using the download button. If you get shape errors in the Dataset and your images files are only as few KBytes small, then you have not successfully downloaded the files.\n",
    "\n",
    "The MNIST Fashion dataset have 10 classes: ['T-shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'].  \n",
    "\n",
    "The training set consists of 60,000 images and the test set consists of 10,000 images. The images are of size [28,28].\n",
    "\n",
    "\n",
    "**Important!**\n",
    "You will need to add code only at locations where a NotImplementedError is raised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\"%matplotlib inline\"</b> is used to plot figures within Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils.utility_functions import datasetFashionMNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits() #mnist dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Handling of the data\n",
    "\n",
    "The following cell creates two instances of \"datasetFashionMNIST\". The \"datasetFashionMNIST\" is a \"torch.utils.data.Dataset\" written for the MNIST Fashion dataset.\n",
    "\n",
    "If you do not use an IFI computer, edit the \"dataPath\" to the location of the MNIST Fashion dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the MNIST Fashion files\n",
    "#dataPath = 'data/MNIST_fashion/'\n",
    "dataPath = '/projects/in5400/MNIST_fashion/'\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = datasetFashionMNIST(dataPath=dataPath, train=True)\n",
    "val_dataset   = datasetFashionMNIST(dataPath=dataPath, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['T-shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "plt.figure(figsize=(18, 16), dpi=80)\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(np.array(train_dataset.labels) == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        img = (train_dataset.images[idx,:]).astype(np.uint8)\n",
    "        img = np.resize(img, (28, 28))   # reshape to 28x28\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To keep track of important parameters, we use dictionary \"config\". When you are done implementing the NotImplementedError's below, you may find it interesting to experiment with different values for the batch size, learning rate and number of epochs trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          'batch_size': 128,\n",
    "          'use_cuda': False,      #True=use Nvidia GPU | False use CPU\n",
    "          'log_interval': 20,     #How often to display (batch) loss during training\n",
    "          'epochs': 20,           #Number of epochs\n",
    "          'learningRate': 0.001\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the data with the instances of \"datasetFashionMNIST\". However, we will for convenience use PyTorch's \"torch.utils.data.DataLoader\" class as it helps us with batching and shuffling of the data. It also makes it possible to use multiple CPU cores/threads to speed up data preprocessing. Your task is to instantiate two data loaders (one for each of the training and validation dataset objects), using PyTorch's dataloader \"torch.utils.data.DataLoader\". Consider if you will use multiple workers and shuffling of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "\n",
    "raise NotImplementedError('Define \\'train_loader\\'.')\n",
    "raise NotImplementedError('Define \\'val_loader\\'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Build the model\n",
    "\n",
    "You are now to define the network architecture. The code provided below defines a fully connected neural network (dense neural network) with two hidden layer of size 128 and 64. However, we encourage you to play with the network configuration.\n",
    "\n",
    "The input has shape [batch size, 28x28]. The 28x28 image size are being concatenated in \"datasetFashionMNIST\". Try to change:\n",
    "- The number of layers\n",
    "- The size of the hidden layers\n",
    "- The activation functions\n",
    "\n",
    "\n",
    "Note that the model inherits from \"torch.nn.Module\", which requires the two class methods \"__init__\" and \"forward\". The former defines the layers used by the model, while the latter defines how the layers are stacked inside the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an instance of Model\n",
    "model = Model()\n",
    "if config['use_cuda'] == True:\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Define optimizer and loss function\n",
    "\n",
    "Instantiate an optimizer, e.g. stochastic gradient descent, from the \"torch.optim\" module (https://pytorch.org/docs/stable/optim.html) with your model. Remember that we have defined \"learning rate\" inside the config-dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of \"torch.optim.SGD\"\n",
    "\n",
    "raise NotImplementedError('Define \\'optimizer\\'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here we want to define the loss function (often called criterion). As we are dealing with a classification problem, the softmax cross entropy loss is an appropriate choice.\n",
    "\n",
    "Hint, have a look here: (https://pytorch.org/docs/stable/nn.html#torch-nn-functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(prediction, labels):\n",
    "    \"\"\"Returns softmax cross entropy loss.\"\"\"\n",
    "    raise NotImplementedError('Define \\'loss\\'.')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Set up the training process and train the model\n",
    "\n",
    "You now have all the building blocks needed to set up the training process. You will implement the function \"run_epoch\" which shall loop though a dataset and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, epoch, data_loader, optimizer, is_training, config):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model        (obj): The neural network model\n",
    "        epoch        (int): The current epoch\n",
    "        data_loader  (obj): A pytorch data loader \"torch.utils.data.DataLoader\"\n",
    "        optimizer    (obj): A pytorch optimizer \"torch.optim\"\n",
    "        is_training (bool): Whether to use train (update) the model/weights or not. \n",
    "        config      (dict): Configuration parameters\n",
    "\n",
    "    Intermediate:\n",
    "        totalLoss: (float): The accumulated loss from all batches. \n",
    "                            Hint: Should be a numpy scalar and not a pytorch scalar\n",
    "\n",
    "    Returns:\n",
    "        loss_avg         (float): The average loss of the dataset\n",
    "        accuracy         (float): The average accuracy of the dataset\n",
    "        confusion_matrix (float): A 10x10 matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_training==True: \n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss       = 0 \n",
    "    correct          = 0 \n",
    "    confusion_matrix = np.zeros(shape=(10,10))\n",
    "    labels_list      = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(data_loader):\n",
    "        if config['use_cuda'] == True:\n",
    "            images = data_batch[0].to('cuda') # send data to GPU\n",
    "            labels = data_batch[1].to('cuda') # send data to GPU\n",
    "        else:\n",
    "            images = data_batch[0]\n",
    "            labels = data_batch[1]\n",
    "\n",
    "        if not is_training:\n",
    "            with torch.no_grad():\n",
    "                raise NotImplementedError('Store the output of a forward pass in \\'prediction\\'.')\n",
    "                raise NotImplementedError('Compute the loss and store it in \\'loss\\'.')\n",
    "                raise NotImplementedError('Update \\'total_loss\\'. Note: It can be beneficial to detach \\'total_loss\\' from the graph, so consider converting the loss to numpy before adding it to \\'total_loss\\'.')\n",
    "            \n",
    "        elif is_training:\n",
    "            raise NotImplementedError('Store the output of a forward pass in \\'prediction\\'.')\n",
    "            raise NotImplementedError('Compute the loss and store it in \\'loss\\'.')\n",
    "            raise NotImplementedError('Update \\'total_loss\\'. Note: It can be beneficial to detach \\'total_loss\\' from the graph, so consider converting the loss to numpy before adding it to \\'total_loss\\'.')\n",
    "\n",
    "            raise NotImplementedError('Do a gradient descent step by 1) setting the gradients to zero, 2) performing backpropagation, and 3) updating the parameters.')\n",
    "            \n",
    "            \n",
    "\n",
    "        # Update the number of correct classifications and the confusion matrix\n",
    "        predicted_label  = prediction.max(1, keepdim=True)[1][:,0]\n",
    "        correct          += predicted_label.eq(labels).cpu().sum().numpy()\n",
    "        confusion_matrix += metrics.confusion_matrix(labels.cpu().numpy(), predicted_label.cpu().numpy(), labels=labels_list)\n",
    "\n",
    "        # Print statistics\n",
    "        #batchSize = len(labels)\n",
    "        if batch_idx % config['log_interval'] == 0:\n",
    "            print(f'Epoch={epoch} | {(batch_idx+1)/len(data_loader)*100:.2f}% | loss = {loss:.5f}')\n",
    "\n",
    "    loss_avg         = total_loss / len(data_loader)\n",
    "    accuracy         = correct / len(data_loader.dataset)\n",
    "    confusion_matrix = confusion_matrix / len(data_loader.dataset)\n",
    "\n",
    "    return loss_avg, accuracy, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here is where the action takes place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "train_loss = np.zeros(shape=config['epochs'])\n",
    "train_acc  = np.zeros(shape=config['epochs'])\n",
    "val_loss   = np.zeros(shape=config['epochs'])\n",
    "val_acc    = np.zeros(shape=config['epochs'])\n",
    "train_confusion_matrix = np.zeros(shape=(10,10,config['epochs']))\n",
    "val_confusion_matrix   = np.zeros(shape=(10,10,config['epochs']))\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    train_loss[epoch], train_acc[epoch], train_confusion_matrix[:,:,epoch] = \\\n",
    "                               run_epoch(model, epoch, train_loader, optimizer, is_training=True, config=config)\n",
    "\n",
    "    val_loss[epoch], val_acc[epoch], val_confusion_matrix[:,:,epoch]     = \\\n",
    "                               run_epoch(model, epoch, val_loader, optimizer, is_training=False, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5. Plot the train and validation losses\n",
    "Plot the loss and the accuracy as a function of epochs to monitor the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and the accuracy in training and validation\n",
    "#plt.figure()\n",
    "plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "# plt.subplots_adjust(hspace=2)\n",
    "ax.plot(train_loss, 'b', label='train loss')\n",
    "ax.plot(val_loss, 'r', label='validation loss')\n",
    "ax.grid()\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "ax.legend(loc='upper right', fontsize=16)\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "ax.plot(train_acc, 'b', label='train accuracy')\n",
    "ax.plot(val_acc, 'r', label='validation accuracy')\n",
    "ax.grid()\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "val_acc_max = np.max(val_acc)\n",
    "val_acc_max_ind = np.argmax(val_acc)\n",
    "plt.axvline(x=val_acc_max_ind, color='g', linestyle='--', label='Highest validation accuracy')\n",
    "plt.title('Highest validation accuracy = %0.1f %%' % (val_acc_max*100), fontsize=16)\n",
    "ax.legend(loc='lower right', fontsize=16)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let us study the accuracy per class on the validation dataset. We use the result from the epoch with highest validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmax(val_acc)\n",
    "class_accuracy = val_confusion_matrix[:,:,ind]\n",
    "for ii in range(len(classes)):\n",
    "    acc = val_confusion_matrix[ii,ii,ind] / np.sum(val_confusion_matrix[ii,:,ind])\n",
    "    print(f'Accuracy of {str(classes[ii]).ljust(15)}: {acc*100:.01f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In order to see how the network learns to distinguish the different classes as the training progresses, we can plot the confusion matrices after each second epoch as heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "epoch_step                  = 2    \n",
    "set_colorbar_max_percentage = 10 \n",
    "    \n",
    "# Plot confusion matrices\n",
    "ticks = np.linspace(0,9,10)\n",
    "gridspec_kwargs = dict(top=0.9, bottom=0.1, left=0.0, right=0.9, wspace=0.5, hspace=0.2)\n",
    "for i in range(0, config['epochs'], epoch_step):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 16), gridspec_kw=gridspec_kwargs)\n",
    "    im = ax1.imshow(val_confusion_matrix[:, :, i]*100)\n",
    "    ax1.set_title(f'Validation: Epoch #{i}', fontsize=18)\n",
    "    ax1.set_xticks(ticks=ticks)\n",
    "    ax1.set_yticks(ticks=ticks)\n",
    "    ax1.set_yticklabels(classes)\n",
    "    im.set_clim(0.0, set_colorbar_max_percentage)\n",
    "    ax1.set_xticklabels(classes, rotation=45)\n",
    "    ax1.set_ylabel('Prediction', fontsize=16)\n",
    "    ax1.set_xlabel('Ground truth', fontsize=16)\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax     = divider.append_axes('right', size='5%', pad=0.15)\n",
    "    f.colorbar(im, cax=cax, orientation='vertical')\n",
    "    \n",
    "    im = ax2.imshow(train_confusion_matrix[:, :, i]*100)\n",
    "    ax2.set_title(f'Train: Epoch #{i}', fontsize=18)\n",
    "    ax2.set_xticks(ticks=ticks)\n",
    "    ax2.set_yticks(ticks=ticks)\n",
    "    ax2.set_yticklabels(classes)\n",
    "    im.set_clim(0.0, set_colorbar_max_percentage)\n",
    "    ax2.set_xticklabels(classes, rotation=45)\n",
    "    ax2.set_ylabel('Prediction', fontsize=16)\n",
    "    ax2.set_xlabel('Ground truth', fontsize=16)\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax     = divider.append_axes('right', size='5%', pad=0.15)\n",
    "    f.colorbar(im, cax=cax, orientation='vertical')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
